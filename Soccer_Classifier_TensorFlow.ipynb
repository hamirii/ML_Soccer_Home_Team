{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple machine learning program on Serie A 2018/19 Season Stats - Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a general function that will take a dataset of football/soccer results and output the number of wins, draws, and losses. Note that the 'data' should be in Panda DataFrame format, with FTR being a category/heading of a column. In our datasets, this is a given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General function that isolates the full time result to see record of wins/losses/draws\n",
    "\n",
    "def winRecords(data, leagueName):\n",
    "    \n",
    "    ftr = data['FTR']\n",
    "\n",
    "    homeWins=0\n",
    "    awayWins=0\n",
    "    draws=0\n",
    "    \n",
    "    for i in range(0, len(ftr.to_numpy())-1):\n",
    "    \n",
    "        if (ftr.to_numpy()[i] == 'H'):\n",
    "            homeWins += 1\n",
    "            #print(j)\n",
    "        elif (ftr.to_numpy()[i] == 'A'):\n",
    "            awayWins += 1\n",
    "        else:\n",
    "            draws +=1\n",
    "\n",
    "    return leagueName, 'HomeWins: %s' % homeWins, 'Away Wins: %s' % awayWins, 'Draws: %s' % draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's read the files and print out the stats for Serie A leagues in several seasons (and the EPL season)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SerieA_15_16', 'HomeWins: 175', 'Away Wins: 109', 'Draws: 95')\n",
      "('SerieA_16_17', 'HomeWins: 183', 'Away Wins: 116', 'Draws: 80')\n",
      "('SerieA_18_19', 'HomeWins: 165', 'Away Wins: 106', 'Draws: 108')\n",
      "('EPL_18_19', 'HomeWins: 181', 'Away Wins: 127', 'Draws: 71')\n"
     ]
    }
   ],
   "source": [
    "print(winRecords(pd.read_csv(\"serieA_season-1516.csv\"), 'SerieA_15_16'))\n",
    "print(winRecords(pd.read_csv(\"serieA_season-1617.csv\"), 'SerieA_16_17'))\n",
    "print(winRecords(pd.read_csv(\"serieA_season-1819.csv\"), 'SerieA_18_19'))\n",
    "print(winRecords(pd.read_csv(\"epl_season-1819.csv\"), 'EPL_18_19'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start training/learning/evaluating/testing our Model\n",
    "\n",
    "Beginning with Serie A 2018/19 dataset, we must first 'clean' the data. This is done by dropping the useless categories. In our case, this is the division and the date of the games. We don't care when the games were played (they're already ordered chronologically), but who played whom and when. We also want to drop the Full Time Home and Away Goals Scored and the Half Time Home & Away Goals and Result categories, as these will obviously influence our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'H', 'D'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSerieA1819 = pd.read_csv(\"serieA_season-1819.csv\")\n",
    "\n",
    "# Dropping the Div and Dates\n",
    "\n",
    "dataSerieA1819 = dataSerieA1819.drop('Div', axis=1)\n",
    "dataSerieA1819 = dataSerieA1819.drop('Date', axis=1)\n",
    "\n",
    "# Dropping the Full Time Home Goals Scored, FTAG\n",
    "\n",
    "dataSerieA1819 = dataSerieA1819.drop('FTAG', axis=1)\n",
    "dataSerieA1819 = dataSerieA1819.drop('FTHG', axis=1)\n",
    "\n",
    "# Dropping the Half Time Home Goals Scored, HTAG, and the Half time result\n",
    "\n",
    "dataSerieA1819 = dataSerieA1819.drop('HTHG', axis=1)\n",
    "dataSerieA1819 = dataSerieA1819.drop('HTAG', axis=1)\n",
    "dataSerieA1819 = dataSerieA1819.drop('HTR', axis=1)\n",
    "\n",
    "dataSerieA1819['FTR'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a look at the column of interest, which is FTR (Full Time Result), and change it to 0s and 1s for our system to be able to classify/predict these outcomes easily. 0 if the Home Team draws or loses, 1 if the Home Team wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the string for Full Time Result (FTR), AKA the outcome to 0s and 1s. 1 if the Home Team wins, 0 if the Away Team won or if a Draw was achieved\n",
    "\n",
    "def fix_outcome(outcome):\n",
    "    if outcome == 'H':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dataSerieA1819['FTR'] = dataSerieA1819['FTR'].apply(fix_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the train_test_split function from the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can Train_Test_Split our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split Data\n",
    "\n",
    "x_data = dataSerieA1819.drop('FTR', axis=1)\n",
    "y_labels = dataSerieA1819['FTR']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_labels, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import TensorFlow package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the feature columns. In this notebook, this is split up into a section for the categorical values, and a section for the continuous/numerical values. This is because they use different lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.feature_columns for Categorical Values\n",
    "\n",
    "HomeTeam = tf.feature_column.categorical_column_with_hash_bucket('HomeTeam', hash_bucket_size=1000)\n",
    "AwayTeam = tf.feature_column.categorical_column_with_hash_bucket('AwayTeam', hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for these values, there is not an 'FTR' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.feature_columns for Numerical/Continuous Values\n",
    "\n",
    "HS   = tf.feature_column.numeric_column('HS')   # Home Shots\n",
    "AS   = tf.feature_column.numeric_column('AS')   # Away Shots\n",
    "HST  = tf.feature_column.numeric_column('HST')  # Home Shots on Target\n",
    "AST  = tf.feature_column.numeric_column('AST')  # Away Shots on Target\n",
    "HF  = tf.feature_column.numeric_column('HF')    # Home Fouls\n",
    "AF  = tf.feature_column.numeric_column('AF')    # Away Fouls\n",
    "HC  = tf.feature_column.numeric_column('HC')    # Home Corners\n",
    "AC  = tf.feature_column.numeric_column('AC')    # Away Corners\n",
    "HY  = tf.feature_column.numeric_column('HY')    # Home Yellow Cards\n",
    "AY  = tf.feature_column.numeric_column('AY')    # Away Yellow Cards\n",
    "HR  = tf.feature_column.numeric_column('HR')    # Home Red Cards\n",
    "AR  = tf.feature_column.numeric_column('AR')    # Away Red Cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile into one features column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile into one features column\n",
    "\n",
    "feature_cols = [HomeTeam,AwayTeam,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tensor flow estimator method, let's build our input function. Note that the batch_size is 100 and the number of epochs is \"None\". The batch size is the number of samples that are processed before updating the model, while the epochs are the number of complete passes through the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an input function\n",
    "\n",
    "input_fnc = tf.estimator.inputs.pandas_input_fn(x=X_train, y=y_train, batch_size=100, num_epochs=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TensorFlow Estimator, with Linear Classifier, let's create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpH1UCp1\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12c98fb10>, '_model_dir': '/var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpH1UCp1', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x12c98ff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x12c98ff10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x12c98ff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x12c98ff10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12ca3f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12ca3f610>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12ca3f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12ca3f610>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpH1UCp1/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.31472, step = 1\n",
      "INFO:tensorflow:global_step/sec: 137.243\n",
      "INFO:tensorflow:loss = 50.365833, step = 101 (0.738 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.78\n",
      "INFO:tensorflow:loss = 38.413895, step = 201 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.157\n",
      "INFO:tensorflow:loss = 45.74372, step = 301 (0.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.398\n",
      "INFO:tensorflow:loss = 35.980263, step = 401 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.298\n",
      "INFO:tensorflow:loss = 41.097588, step = 501 (0.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.804\n",
      "INFO:tensorflow:loss = 37.569267, step = 601 (0.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.589\n",
      "INFO:tensorflow:loss = 41.276, step = 701 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.747\n",
      "INFO:tensorflow:loss = 54.443317, step = 801 (0.664 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.199\n",
      "INFO:tensorflow:loss = 42.758602, step = 901 (0.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.107\n",
      "INFO:tensorflow:loss = 52.27602, step = 1001 (0.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.942\n",
      "INFO:tensorflow:loss = 36.061348, step = 1101 (0.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.888\n",
      "INFO:tensorflow:loss = 35.91867, step = 1201 (0.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.41\n",
      "INFO:tensorflow:loss = 43.46231, step = 1301 (0.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.204\n",
      "INFO:tensorflow:loss = 40.021698, step = 1401 (0.747 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.663\n",
      "INFO:tensorflow:loss = 41.14967, step = 1501 (0.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.7\n",
      "INFO:tensorflow:loss = 41.084923, step = 1601 (0.762 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.456\n",
      "INFO:tensorflow:loss = 38.684017, step = 1701 (0.785 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.132\n",
      "INFO:tensorflow:loss = 45.380245, step = 1801 (0.783 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.917\n",
      "INFO:tensorflow:loss = 42.51356, step = 1901 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.627\n",
      "INFO:tensorflow:loss = 40.53567, step = 2001 (0.759 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.07\n",
      "INFO:tensorflow:loss = 35.571808, step = 2101 (0.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.794\n",
      "INFO:tensorflow:loss = 41.97688, step = 2201 (0.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.211\n",
      "INFO:tensorflow:loss = 42.415596, step = 2301 (0.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.953\n",
      "INFO:tensorflow:loss = 40.286102, step = 2401 (0.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.928\n",
      "INFO:tensorflow:loss = 39.10371, step = 2501 (0.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 46.10028, step = 2601 (0.783 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.285\n",
      "INFO:tensorflow:loss = 44.507694, step = 2701 (0.613 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.915\n",
      "INFO:tensorflow:loss = 42.481216, step = 2801 (0.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.608\n",
      "INFO:tensorflow:loss = 38.050945, step = 2901 (0.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.336\n",
      "INFO:tensorflow:loss = 52.51755, step = 3001 (0.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.025\n",
      "INFO:tensorflow:loss = 34.82693, step = 3101 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.239\n",
      "INFO:tensorflow:loss = 40.723103, step = 3201 (0.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.891\n",
      "INFO:tensorflow:loss = 44.055832, step = 3301 (0.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.533\n",
      "INFO:tensorflow:loss = 49.15535, step = 3401 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.745\n",
      "INFO:tensorflow:loss = 46.509056, step = 3501 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.452\n",
      "INFO:tensorflow:loss = 42.75459, step = 3601 (0.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.211\n",
      "INFO:tensorflow:loss = 41.73696, step = 3701 (0.595 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.448\n",
      "INFO:tensorflow:loss = 45.590034, step = 3801 (0.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4502\n",
      "INFO:tensorflow:loss = 37.729767, step = 3901 (1.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.877\n",
      "INFO:tensorflow:loss = 38.480835, step = 4001 (1.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.433\n",
      "INFO:tensorflow:loss = 44.234863, step = 4101 (0.952 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.26\n",
      "INFO:tensorflow:loss = 35.8228, step = 4201 (0.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.775\n",
      "INFO:tensorflow:loss = 34.890522, step = 4301 (0.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.548\n",
      "INFO:tensorflow:loss = 36.408176, step = 4401 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.274\n",
      "INFO:tensorflow:loss = 44.762825, step = 4501 (0.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.742\n",
      "INFO:tensorflow:loss = 44.173637, step = 4601 (0.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.794\n",
      "INFO:tensorflow:loss = 34.295044, step = 4701 (0.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.211\n",
      "INFO:tensorflow:loss = 42.618683, step = 4801 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.332\n",
      "INFO:tensorflow:loss = 42.0052, step = 4901 (0.614 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpH1UCp1/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 39.62018.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearClassifier at 0x110268bd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Linear Classifier to create a model with tf.Estimator\n",
    "\n",
    "model = tf.estimator.LinearClassifier(feature_columns=feature_cols)\n",
    "\n",
    "model.train(input_fn=input_fnc, steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using model.predict and a new input function pred_fn, generate predictions. Use list() to put those predictions in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x1187f5650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x1187f5650>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x1187f5650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x1187f5650>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12f415d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12f415d90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12f415d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12f415d90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpH1UCp1/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# Now to make 'predictions'\n",
    "\n",
    "pred_fn = tf.estimator.inputs.pandas_input_fn(x=X_test, batch_size = len(X_test), shuffle=False)\n",
    "predictions = list(model.predict(input_fn=pred_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the predictions look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all_class_ids': array([0, 1], dtype=int32), 'all_classes': array(['0', '1'], dtype=object), 'probabilities': array([0.64962864, 0.35037133], dtype=float32), 'classes': array(['0'], dtype=object), 'logistic': array([0.35037136], dtype=float32), 'logits': array([-0.61740726], dtype=float32), 'class_ids': array([0])}\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting. Each prediction has a logit, logistic, probability, class_id, etc. associated with it. We care most about the class_ids as that will indicate whether the Home Team won the match or not. In this model, draws are considered losses. Iterate over the list of predictions, appending only the relevant category, class_ids, to a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds= []\n",
    "for pred in predictions:\n",
    "    final_preds.append(pred['class_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(final_preds[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to evaluate, using sklearn metrics, how accurate our model actually is! Import Classification_report from the sklearn metrics package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.70      0.68        61\n",
      "           1       0.62      0.57      0.59        53\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       114\n",
      "   macro avg       0.64      0.64      0.64       114\n",
      "weighted avg       0.64      0.64      0.64       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Classification report from SkLearn-Metrics to evaluate the performance of the test model\n",
    "\n",
    "print(classification_report(y_test, final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up the code, I create separate functions for each process. cleanData drops the irrelevant columns and/or rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the training/learning/evaluating/testing Machine Learning Method! First, we need to clean the data.\n",
    "# Note: This script was written assuming all the data files will have the same default columns. This is true for the datasets that I've downloaded from the internet.\n",
    "# I removed (in some of these files) the betting odds, as I figure these may influence results in ways outside of the determinants of the game. \n",
    "# Odds are set by bookkeepers, based on their own analysis.\n",
    "\n",
    "\n",
    "def cleanData(dataFileCSV):\n",
    "\n",
    "    data = pd.read_csv(dataFileCSV)\n",
    "\n",
    "    # Dropping the Div and Dates\n",
    "\n",
    "    data = data.drop('Div', axis=1)\n",
    "    data = data.drop('Date', axis=1)\n",
    "\n",
    "    # Dropping the Full Time Home Goals Scored, FTAG\n",
    "\n",
    "    data = data.drop('FTAG', axis=1)\n",
    "    data = data.drop('FTHG', axis=1)\n",
    "\n",
    "    # Dropping the Half Time Home Goals Scored, HTAG, and the Half time result\n",
    "\n",
    "    data = data.drop('HTHG', axis=1)\n",
    "    data = data.drop('HTAG', axis=1)\n",
    "    data = data.drop('HTR', axis=1)\n",
    "\n",
    "    data['FTR'].unique()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _fix outcome_ function stays the same as above, so I won't include it here. _trainModel_ takes in clean data, creates and trains a model. This function returns a _trained model_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(data):\n",
    "    data['FTR'] = data['FTR'].apply(fix_outcome)\n",
    "\n",
    "    # Train Test Split Data\n",
    "\n",
    "    x_data = data.drop('FTR', axis=1)\n",
    "    y_labels = data['FTR']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_data, y_labels, test_size=0.3, random_state=101)\n",
    "\n",
    "\n",
    "    # Create tf.feature_columns for Categorical Values\n",
    "\n",
    "    HomeTeam = tf.feature_column.categorical_column_with_hash_bucket('HomeTeam', hash_bucket_size=1000)\n",
    "    AwayTeam = tf.feature_column.categorical_column_with_hash_bucket('AwayTeam', hash_bucket_size=1000)\n",
    "    \n",
    "\n",
    "    # Create tf.feature_columns for Numerical/Continuous Values\n",
    "\n",
    "    HS   = tf.feature_column.numeric_column('HS')   # Home Shots\n",
    "    AS   = tf.feature_column.numeric_column('AS')   # Away Shots\n",
    "    HST  = tf.feature_column.numeric_column('HST')  # Home Shots on Target\n",
    "    AST  = tf.feature_column.numeric_column('AST')  # Away Shots on Target\n",
    "    HF  = tf.feature_column.numeric_column('HF')    # Home Fouls\n",
    "    AF  = tf.feature_column.numeric_column('AF')    # Away Fouls\n",
    "    HC  = tf.feature_column.numeric_column('HC')    # Home Corners\n",
    "    AC  = tf.feature_column.numeric_column('AC')    # Away Corners\n",
    "    HY  = tf.feature_column.numeric_column('HY')    # Home Yellow Cards\n",
    "    AY  = tf.feature_column.numeric_column('AY')    # Away Yellow Cards\n",
    "    HR  = tf.feature_column.numeric_column('HR')    # Home Red Cards\n",
    "    AR  = tf.feature_column.numeric_column('AR')    # Away Red Cards\n",
    "\n",
    "\n",
    "    # Compile into one features column\n",
    "    \n",
    "    feature_cols = [HomeTeam,AwayTeam,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR]\n",
    "\n",
    "    # Building an input function\n",
    "\n",
    "    input_fnc = tf.estimator.inputs.pandas_input_fn(x=X_train, y=y_train, batch_size=100, num_epochs=None, shuffle=True)\n",
    "\n",
    "    # Using Linear Classifier to create a model with tf.Estimator\n",
    "\n",
    "    model = tf.estimator.LinearClassifier(feature_columns=feature_cols)\n",
    "\n",
    "    model.train(input_fn=input_fnc, steps=5000)\n",
    "\n",
    "    return model, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function that makes predictions and evaluates/tests the model using sklearn-classification-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to make 'predictions'\n",
    "\n",
    "def predict(data):\n",
    "\n",
    "    model, X_test, y_test = trainModel(data)\n",
    "\n",
    "    pred_fn = tf.estimator.inputs.pandas_input_fn(x=X_test, batch_size = len(X_test), shuffle=False)\n",
    "    predictions = list(model.predict(input_fn=pred_fn))\n",
    "\n",
    "    print(predictions[0])\n",
    "\n",
    "    final_preds= []\n",
    "    for pred in predictions:\n",
    "        final_preds.append(pred['class_ids'][0])\n",
    "\n",
    "    print(final_preds[:10])\n",
    "\n",
    "    # Import Classification report from SkLearn-Metrics to evaluate the performance of the test model\n",
    "\n",
    "    print(classification_report(y_test, final_preds))\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a main() function that compiles all these other functions so that we only have to write in one function every time we want to test new data! This is fairly reminiscent of OOP principles, something I'd normally do when I'm using Java or C. I'm not sure yet whether it's better notation to just have one function and have all these sub-functions as sub-functions, or just place them as lines of code in one large main function, or to have them split up as such. For now, I'll keep them split up. At the very least, it helps break down the code and make it easier to comprehend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling everything into a 'main' function - this is reminiscent of OOP principles, something I'd do in Java - but it's nice to keep things compact, even in Python\n",
    "\n",
    "def main(data):\n",
    "    return predict(cleanData(data))\n",
    "\n",
    "# I suppose I could very easily just create one main function and wrap the other 'predict' and 'cleanData' methods into the main function, but I'm still undecided as to whether that is better documentation than splitting it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same (serieA_season-1819.csv) data as above, let's see if we get the same results. Hint: if we return the right datatypes for each function, we should get the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpep1oM5\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12da22990>, '_model_dir': '/var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpep1oM5', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x12d615d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x12d615d10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x12d615d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x12d615d10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12d1c9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12d1c9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12d1c9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x12d1c9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpep1oM5/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.31472, step = 1\n",
      "INFO:tensorflow:global_step/sec: 133.385\n",
      "INFO:tensorflow:loss = 41.16319, step = 101 (0.755 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.128\n",
      "INFO:tensorflow:loss = 47.96904, step = 201 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.277\n",
      "INFO:tensorflow:loss = 40.89923, step = 301 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.255\n",
      "INFO:tensorflow:loss = 45.869724, step = 401 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.918\n",
      "INFO:tensorflow:loss = 36.5951, step = 501 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.829\n",
      "INFO:tensorflow:loss = 41.73495, step = 601 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.339\n",
      "INFO:tensorflow:loss = 40.52789, step = 701 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.824\n",
      "INFO:tensorflow:loss = 43.05406, step = 801 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.213\n",
      "INFO:tensorflow:loss = 43.39981, step = 901 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.412\n",
      "INFO:tensorflow:loss = 38.9615, step = 1001 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.588\n",
      "INFO:tensorflow:loss = 42.425873, step = 1101 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.417\n",
      "INFO:tensorflow:loss = 53.835808, step = 1201 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.51\n",
      "INFO:tensorflow:loss = 40.034885, step = 1301 (0.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.532\n",
      "INFO:tensorflow:loss = 49.000824, step = 1401 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.699\n",
      "INFO:tensorflow:loss = 38.665062, step = 1501 (0.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.596\n",
      "INFO:tensorflow:loss = 36.8578, step = 1601 (0.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.623\n",
      "INFO:tensorflow:loss = 54.384384, step = 1701 (0.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.898\n",
      "INFO:tensorflow:loss = 40.16256, step = 1801 (0.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.433\n",
      "INFO:tensorflow:loss = 36.958263, step = 1901 (0.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.733\n",
      "INFO:tensorflow:loss = 42.62551, step = 2001 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.772\n",
      "INFO:tensorflow:loss = 42.62741, step = 2101 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.4\n",
      "INFO:tensorflow:loss = 45.187923, step = 2201 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.409\n",
      "INFO:tensorflow:loss = 34.25932, step = 2301 (0.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.894\n",
      "INFO:tensorflow:loss = 31.076458, step = 2401 (0.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.588\n",
      "INFO:tensorflow:loss = 48.24611, step = 2501 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.274\n",
      "INFO:tensorflow:loss = 45.18423, step = 2601 (0.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.283\n",
      "INFO:tensorflow:loss = 38.073223, step = 2701 (0.613 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.358\n",
      "INFO:tensorflow:loss = 31.941275, step = 2801 (0.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.4257\n",
      "INFO:tensorflow:loss = 37.725952, step = 2901 (1.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.018\n",
      "INFO:tensorflow:loss = 38.785637, step = 3001 (0.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.49\n",
      "INFO:tensorflow:loss = 49.30315, step = 3101 (0.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.209\n",
      "INFO:tensorflow:loss = 49.622143, step = 3201 (0.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.699\n",
      "INFO:tensorflow:loss = 41.27787, step = 3301 (0.755 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.156\n",
      "INFO:tensorflow:loss = 45.300358, step = 3401 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.348\n",
      "INFO:tensorflow:loss = 38.78542, step = 3501 (0.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.218\n",
      "INFO:tensorflow:loss = 33.86581, step = 3601 (0.769 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.606\n",
      "INFO:tensorflow:loss = 39.29224, step = 3701 (0.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.719\n",
      "INFO:tensorflow:loss = 43.03564, step = 3801 (0.819 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.875\n",
      "INFO:tensorflow:loss = 42.21216, step = 3901 (0.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.44\n",
      "INFO:tensorflow:loss = 41.79215, step = 4001 (0.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.851\n",
      "INFO:tensorflow:loss = 44.4633, step = 4101 (0.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.706\n",
      "INFO:tensorflow:loss = 49.832386, step = 4201 (0.777 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.791\n",
      "INFO:tensorflow:loss = 40.60818, step = 4301 (0.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.741\n",
      "INFO:tensorflow:loss = 42.610725, step = 4401 (0.758 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.408\n",
      "INFO:tensorflow:loss = 45.836044, step = 4501 (0.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.443\n",
      "INFO:tensorflow:loss = 37.681767, step = 4601 (0.744 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 170.074\n",
      "INFO:tensorflow:loss = 35.524277, step = 4701 (0.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.36\n",
      "INFO:tensorflow:loss = 44.550823, step = 4801 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.709\n",
      "INFO:tensorflow:loss = 43.118744, step = 4901 (0.782 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpep1oM5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 49.868595.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x10fbbdcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x10fbbdcd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x10fbbdcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x10fbbdcd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x10fbbddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x10fbbddd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x10fbbddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x10fbbddd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpep1oM5/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'all_class_ids': array([0, 1], dtype=int32), 'all_classes': array(['0', '1'], dtype=object), 'probabilities': array([0.6462353 , 0.35376468], dtype=float32), 'classes': array(['0'], dtype=object), 'logistic': array([0.3537647], dtype=float32), 'logits': array([-0.60253173], dtype=float32), 'class_ids': array([0])}\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.70      0.68        61\n",
      "           1       0.62      0.57      0.59        53\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       114\n",
      "   macro avg       0.64      0.64      0.64       114\n",
      "weighted avg       0.64      0.64      0.64       114\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main('serieA_season-1819.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! We do! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
