{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple machine learning program on Serie A 2018/19 Season Stats - Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a general function that will take a dataset of football/soccer results and output the number of wins, draws, and losses. Note that the 'data' should be in Panda DataFrame format, with FTR being a category/heading of a column. In our datasets, this is a given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General function that isolates the full time result to see record of wins/losses/draws\n",
    "\n",
    "def winRecords(data, leagueName):\n",
    "    \n",
    "    ftr = data['FTR']\n",
    "\n",
    "    homeWins=0\n",
    "    awayWins=0\n",
    "    draws=0\n",
    "    \n",
    "    for i in range(0, len(ftr.to_numpy())-1):\n",
    "    \n",
    "        if (ftr.to_numpy()[i] == 'H'):\n",
    "            homeWins += 1\n",
    "            #print(j)\n",
    "        elif (ftr.to_numpy()[i] == 'A'):\n",
    "            awayWins += 1\n",
    "        else:\n",
    "            draws +=1\n",
    "\n",
    "    return leagueName, 'HomeWins: %s' % homeWins, 'Away Wins: %s' % awayWins, 'Draws: %s' % draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's read the files and print out the stats for Serie A leagues in several seasons (and the EPL season)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SerieA_15_16', 'HomeWins: 175', 'Away Wins: 109', 'Draws: 95')\n",
      "('SerieA_16_17', 'HomeWins: 183', 'Away Wins: 116', 'Draws: 80')\n",
      "('SerieA_18_19', 'HomeWins: 165', 'Away Wins: 106', 'Draws: 108')\n",
      "('EPL_18_19', 'HomeWins: 181', 'Away Wins: 127', 'Draws: 71')\n"
     ]
    }
   ],
   "source": [
    "print(winRecords(pd.read_csv(\"serieA_season-1516.csv\"), 'SerieA_15_16'))\n",
    "print(winRecords(pd.read_csv(\"serieA_season-1617.csv\"), 'SerieA_16_17'))\n",
    "print(winRecords(pd.read_csv(\"serieA_season-1819.csv\"), 'SerieA_18_19'))\n",
    "print(winRecords(pd.read_csv(\"epl_season-1819.csv\"), 'EPL_18_19'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start training/learning/evaluating/testing our Model\n",
    "\n",
    "Beginning with Serie A 2018/19 dataset, we must first 'clean' the data. This is done by dropping the useless categories. In our case, this is the division and the date of the games. We don't care when the games were played (they're already ordered chronologically), but who played whom and when. We also want to drop the Full Time Home and Away Goals Scored and the Half Time Home & Away Goals and Result categories, as these will obviously influence our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'H', 'D'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSerieA1819 = pd.read_csv(\"serieA_season-1819.csv\")\n",
    "\n",
    "# Dropping the Div and Dates\n",
    "\n",
    "dataSerieA1819 = dataSerieA1819.drop('Div', axis=1)\n",
    "dataSerieA1819 = dataSerieA1819.drop('Date', axis=1)\n",
    "\n",
    "# Dropping the Full Time Home Goals Scored, FTAG\n",
    "\n",
    "dataSerieA1819 = dataSerieA1819.drop('FTAG', axis=1)\n",
    "dataSerieA1819 = dataSerieA1819.drop('FTHG', axis=1)\n",
    "\n",
    "# Dropping the Half Time Home Goals Scored, HTAG, and the Half time result\n",
    "\n",
    "dataSerieA1819 = dataSerieA1819.drop('HTHG', axis=1)\n",
    "dataSerieA1819 = dataSerieA1819.drop('HTAG', axis=1)\n",
    "dataSerieA1819 = dataSerieA1819.drop('HTR', axis=1)\n",
    "\n",
    "dataSerieA1819['FTR'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a look at the column of interest, which is FTR (Full Time Result), and change it to 0s and 1s for our system to be able to classify/predict these outcomes easily. 0 if the Home Team draws or loses, 1 if the Home Team wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the string for Full Time Result (FTR), AKA the outcome to 0s and 1s. 1 if the Home Team wins, 0 if the Away Team won or if a Draw was achieved\n",
    "\n",
    "def fix_outcome(outcome):\n",
    "    if outcome == 'H':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dataSerieA1819['FTR'] = dataSerieA1819['FTR'].apply(fix_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the train_test_split function from the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can Train_Test_Split our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split Data\n",
    "\n",
    "x_data = dataSerieA1819.drop('FTR', axis=1)\n",
    "y_labels = dataSerieA1819['FTR']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_labels, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import TensorFlow package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the feature columns. In this notebook, this is split up into a section for the categorical values, and a section for the continuous/numerical values. This is because they use different lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.feature_columns for Categorical Values\n",
    "\n",
    "HomeTeam = tf.feature_column.categorical_column_with_hash_bucket('HomeTeam', hash_bucket_size=1000)\n",
    "AwayTeam = tf.feature_column.categorical_column_with_hash_bucket('AwayTeam', hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for these values, there is not an 'FTR' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.feature_columns for Numerical/Continuous Values\n",
    "\n",
    "HS   = tf.feature_column.numeric_column('HS')   # Home Shots\n",
    "AS   = tf.feature_column.numeric_column('AS')   # Away Shots\n",
    "HST  = tf.feature_column.numeric_column('HST')  # Home Shots on Target\n",
    "AST  = tf.feature_column.numeric_column('AST')  # Away Shots on Target\n",
    "HF  = tf.feature_column.numeric_column('HF')    # Home Fouls\n",
    "AF  = tf.feature_column.numeric_column('AF')    # Away Fouls\n",
    "HC  = tf.feature_column.numeric_column('HC')    # Home Corners\n",
    "AC  = tf.feature_column.numeric_column('AC')    # Away Corners\n",
    "HY  = tf.feature_column.numeric_column('HY')    # Home Yellow Cards\n",
    "AY  = tf.feature_column.numeric_column('AY')    # Away Yellow Cards\n",
    "HR  = tf.feature_column.numeric_column('HR')    # Home Red Cards\n",
    "AR  = tf.feature_column.numeric_column('AR')    # Away Red Cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile into one features column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile into one features column\n",
    "\n",
    "feature_cols = [HomeTeam,AwayTeam,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tensor flow estimator method, let's build our input function. Note that the batch_size is 100 and the number of epochs is \"None\". The batch size is the number of samples that are processed before updating the model, while the epochs are the number of complete passes through the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an input function\n",
    "\n",
    "input_fnc = tf.estimator.inputs.pandas_input_fn(x=X_train, y=y_train, batch_size=100, num_epochs=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TensorFlow Estimator, with Linear Classifier, let's create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 20:28:28.041749 140735565427584 estimator.py:1811] Using temporary folder as model directory: /var/folders/tw/j24f0q85731142mf21hrd98c0000gn/T/tmpw9otpb5m\n",
      "W0917 20:28:28.087664 140735565427584 deprecation.py:323] From //anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0917 20:28:28.114566 140735565427584 deprecation.py:323] From //anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0917 20:28:28.118185 140735565427584 deprecation.py:323] From //anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0917 20:28:28.563650 140735565427584 deprecation.py:323] From //anaconda3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0917 20:28:28.809772 140735565427584 deprecation.py:323] From //anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0917 20:28:31.441477 140735565427584 deprecation.py:323] From //anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearClassifier at 0x1a481d0588>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Linear Classifier to create a model with tf.Estimator\n",
    "\n",
    "model = tf.estimator.LinearClassifier(feature_columns=feature_cols)\n",
    "\n",
    "model.train(input_fn=input_fnc, steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using model.predict and a new input function pred_fn, generate predictions. Use list() to put those predictions in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to make 'predictions'\n",
    "\n",
    "pred_fn = tf.estimator.inputs.pandas_input_fn(x=X_test, batch_size = len(X_test), shuffle=False)\n",
    "predictions = list(model.predict(input_fn=pred_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the predictions look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting. Each prediction has a logit, logistic, probability, class_id, etc. associated with it. We care most about the class_ids as that will indicate whether the Home Team won the match or not. In this model, draws are considered losses. Iterate over the list of predictions, appending only the relevant category, class_ids, to a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds= []\n",
    "for pred in predictions:\n",
    "    final_preds.append(pred['class_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_preds[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to evaluate, using sklearn metrics, how accurate our model actually is! Import Classification_report from the sklearn metrics package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Classification report from SkLearn-Metrics to evaluate the performance of the test model\n",
    "\n",
    "print(classification_report(y_test, final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up the code, I create separate functions for each process. cleanData drops the irrelevant columns and/or rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the training/learning/evaluating/testing Machine Learning Method! First, we need to clean the data.\n",
    "# Note: This script was written assuming all the data files will have the same default columns. This is true for the datasets that I've downloaded from the internet.\n",
    "# I removed (in some of these files) the betting odds, as I figure these may influence results in ways outside of the determinants of the game. \n",
    "# Odds are set by bookkeepers, based on their own analysis.\n",
    "\n",
    "\n",
    "def cleanData(dataFileCSV):\n",
    "\n",
    "    data = pd.read_csv(dataFileCSV)\n",
    "\n",
    "    # Dropping the Div and Dates\n",
    "\n",
    "    data = data.drop('Div', axis=1)\n",
    "    data = data.drop('Date', axis=1)\n",
    "\n",
    "    # Dropping the Full Time Home Goals Scored, FTAG\n",
    "\n",
    "    data = data.drop('FTAG', axis=1)\n",
    "    data = data.drop('FTHG', axis=1)\n",
    "\n",
    "    # Dropping the Half Time Home Goals Scored, HTAG, and the Half time result\n",
    "\n",
    "    data = data.drop('HTHG', axis=1)\n",
    "    data = data.drop('HTAG', axis=1)\n",
    "    data = data.drop('HTR', axis=1)\n",
    "\n",
    "    data['FTR'].unique()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _fix outcome_ function stays the same as above, so I won't include it here. _trainModel_ takes in clean data, creates and trains a model. This function returns a _trained model_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(data):\n",
    "    data['FTR'] = data['FTR'].apply(fix_outcome)\n",
    "\n",
    "    # Train Test Split Data\n",
    "\n",
    "    x_data = data.drop('FTR', axis=1)\n",
    "    y_labels = data['FTR']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_data, y_labels, test_size=0.3, random_state=101)\n",
    "\n",
    "\n",
    "    # Create tf.feature_columns for Categorical Values\n",
    "\n",
    "    HomeTeam = tf.feature_column.categorical_column_with_hash_bucket('HomeTeam', hash_bucket_size=1000)\n",
    "    AwayTeam = tf.feature_column.categorical_column_with_hash_bucket('AwayTeam', hash_bucket_size=1000)\n",
    "    \n",
    "\n",
    "    # Create tf.feature_columns for Numerical/Continuous Values\n",
    "\n",
    "    HS   = tf.feature_column.numeric_column('HS')   # Home Shots\n",
    "    AS   = tf.feature_column.numeric_column('AS')   # Away Shots\n",
    "    HST  = tf.feature_column.numeric_column('HST')  # Home Shots on Target\n",
    "    AST  = tf.feature_column.numeric_column('AST')  # Away Shots on Target\n",
    "    HF  = tf.feature_column.numeric_column('HF')    # Home Fouls\n",
    "    AF  = tf.feature_column.numeric_column('AF')    # Away Fouls\n",
    "    HC  = tf.feature_column.numeric_column('HC')    # Home Corners\n",
    "    AC  = tf.feature_column.numeric_column('AC')    # Away Corners\n",
    "    HY  = tf.feature_column.numeric_column('HY')    # Home Yellow Cards\n",
    "    AY  = tf.feature_column.numeric_column('AY')    # Away Yellow Cards\n",
    "    HR  = tf.feature_column.numeric_column('HR')    # Home Red Cards\n",
    "    AR  = tf.feature_column.numeric_column('AR')    # Away Red Cards\n",
    "\n",
    "\n",
    "    # Compile into one features column\n",
    "    \n",
    "    feature_cols = [HomeTeam,AwayTeam,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR]\n",
    "\n",
    "    # Building an input function\n",
    "\n",
    "    input_fnc = tf.estimator.inputs.pandas_input_fn(x=X_train, y=y_train, batch_size=100, num_epochs=None, shuffle=True)\n",
    "\n",
    "    # Using Linear Classifier to create a model with tf.Estimator\n",
    "\n",
    "    model = tf.estimator.LinearClassifier(feature_columns=feature_cols)\n",
    "\n",
    "    model.train(input_fn=input_fnc, steps=5000)\n",
    "\n",
    "    return model, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function that makes predictions and evaluates/tests the model using sklearn-classification-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to make 'predictions'\n",
    "\n",
    "def predict(data):\n",
    "\n",
    "    model, X_test, y_test = trainModel(data)\n",
    "\n",
    "    pred_fn = tf.estimator.inputs.pandas_input_fn(x=X_test, batch_size = len(X_test), shuffle=False)\n",
    "    predictions = list(model.predict(input_fn=pred_fn))\n",
    "\n",
    "    print(predictions[0])\n",
    "\n",
    "    final_preds= []\n",
    "    for pred in predictions:\n",
    "        final_preds.append(pred['class_ids'][0])\n",
    "\n",
    "    print(final_preds[:10])\n",
    "\n",
    "    # Import Classification report from SkLearn-Metrics to evaluate the performance of the test model\n",
    "\n",
    "    print(classification_report(y_test, final_preds))\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a main() function that compiles all these other functions so that we only have to write in one function every time we want to test new data! This is fairly reminiscent of OOP principles, something I'd normally do when I'm using Java or C. I'm not sure yet whether it's better notation to just have one function and have all these sub-functions as sub-functions, or just place them as lines of code in one large main function, or to have them split up as such. For now, I'll keep them split up. At the very least, it helps break down the code and make it easier to comprehend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling everything into a 'main' function - this is reminiscent of OOP principles, something I'd do in Java - but it's nice to keep things compact, even in Python\n",
    "\n",
    "def main(data):\n",
    "    return predict(cleanData(data))\n",
    "\n",
    "# I suppose I could very easily just create one main function and wrap the other 'predict' and 'cleanData' methods into the main function, but I'm still undecided as to whether that is better documentation than splitting it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using the same (serieA_season-1819.csv) data as above, let's see if we get the same results. Hint: if we return the right datatypes for each function, we should."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
